{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ LSTM Christian Text Generator - Full Training (40 Epochs)\n",
    "\n",
    "**Setup:**\n",
    "1. Runtime ‚Üí Change runtime type ‚Üí **T4 GPU**\n",
    "2. Run all cells\n",
    "3. ~2-3 hours training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£ Check GPU\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2Ô∏è‚É£ Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "output_dir = '/content/drive/MyDrive/everyday-christian-model'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Models will save to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3Ô∏è‚É£ Download training data from YOUR GitHub\n",
    "!wget -q https://raw.githubusercontent.com/elev8tion/edc/main/assets/training_data/lstm_training_data.txt -O /content/training_data.txt\n",
    "!ls -lh /content/training_data.txt\n",
    "print(\"‚úÖ Training data downloaded from GitHub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4Ô∏è‚É£ Configuration\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# FULL CAPACITY - NO COMPROMISES\n",
    "SEQ_LENGTH = 100\n",
    "BATCH_SIZE = 256\n",
    "BUFFER_SIZE = 10000\n",
    "EMBEDDING_DIM = 256\n",
    "RNN_UNITS = 1024\n",
    "EPOCHS = 40  # FULL 40 EPOCHS\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "print(\"üìã FULL Training Configuration:\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  RNN Units: {RNN_UNITS}\")\n",
    "print(f\"  Embedding Dim: {EMBEDDING_DIM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5Ô∏è‚É£ Prepare data\n",
    "def prepare_data():\n",
    "    print(\"üìñ Loading training data...\")\n",
    "    with open('/content/training_data.txt', 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(text):,} characters\")\n",
    "    \n",
    "    # Create vocabulary\n",
    "    vocab = sorted(set(text))\n",
    "    vocab = ['<PAD>', '<START>', '<END>'] + vocab\n",
    "    \n",
    "    char2idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "    idx2char = {idx: char for idx, char in enumerate(vocab)}\n",
    "    \n",
    "    print(f\"üìù Vocabulary size: {len(vocab)}\")\n",
    "    \n",
    "    # Save vocabulary\n",
    "    vocab_path = f'{output_dir}/char_vocab.txt'\n",
    "    with open(vocab_path, 'w', encoding='utf-8') as f:\n",
    "        for char in vocab:\n",
    "            f.write(f\"{char}\\n\")\n",
    "    \n",
    "    # Create sequences\n",
    "    text_as_int = np.array([char2idx.get(c, 0) for c in text])\n",
    "    char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "    sequences = char_dataset.batch(SEQ_LENGTH + 1, drop_remainder=True)\n",
    "    \n",
    "    def split_input_target(chunk):\n",
    "        input_text = chunk[:-1]\n",
    "        target_text = chunk[1:]\n",
    "        return input_text, target_text\n",
    "    \n",
    "    dataset = sequences.map(split_input_target)\n",
    "    dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "    dataset = dataset.cache().prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset, len(vocab), char2idx, idx2char\n",
    "\n",
    "dataset, vocab_size, char2idx, idx2char = prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6Ô∏è‚É£ Build PROPER model (NO batch_input_shape error!)\n",
    "def build_model(vocab_size, embedding_dim, rnn_units):\n",
    "    model = tf.keras.Sequential([\n",
    "        # FIXED: No batch_input_shape argument\n",
    "        tf.keras.layers.Embedding(\n",
    "            vocab_size,\n",
    "            embedding_dim\n",
    "        ),\n",
    "        tf.keras.layers.LSTM(\n",
    "            rnn_units,\n",
    "            return_sequences=True,\n",
    "            recurrent_initializer='glorot_uniform',\n",
    "            recurrent_dropout=0.1\n",
    "        ),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.LSTM(\n",
    "            rnn_units // 2,\n",
    "            return_sequences=True,\n",
    "            recurrent_initializer='glorot_uniform',\n",
    "            recurrent_dropout=0.1\n",
    "        ),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(\n",
    "            vocab_size,\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(0.01)\n",
    "        )\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "print(\"üèóÔ∏è Building LSTM model...\")\n",
    "model = build_model(vocab_size, EMBEDDING_DIM, RNN_UNITS)\n",
    "\n",
    "# Compile\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7Ô∏è‚É£ TRAIN - FULL 40 EPOCHS\n",
    "print(\"üéì Starting FULL training...\")\n",
    "print(\"‚è±Ô∏è Estimated time: 2-3 hours\")\n",
    "print(\"‚òï Colab will keep running\\n\")\n",
    "\n",
    "# Callbacks\n",
    "checkpoint_path = f\"{output_dir}/checkpoint-{{epoch:02d}}\"\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        save_weights_only=True,\n",
    "        save_freq='epoch',\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='loss',\n",
    "        patience=5,\n",
    "        min_delta=0.001,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-5,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# TRAIN!\n",
    "start_time = time.time()\n",
    "history = model.fit(\n",
    "    dataset,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "training_time = (time.time() - start_time) / 3600\n",
    "print(f\"\\n‚úÖ Training complete in {training_time:.2f} hours!\")\n",
    "print(f\"Final Loss: {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"Final Accuracy: {history.history['accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8Ô∏è‚É£ Convert to TFLite for iOS\n",
    "print(\"üîÑ Converting to TFLite...\")\n",
    "\n",
    "# Save weights\n",
    "model.save_weights(f'{output_dir}/final_weights.h5')\n",
    "\n",
    "# Rebuild for inference (batch size 1)\n",
    "inference_model = build_model(vocab_size, EMBEDDING_DIM, RNN_UNITS)\n",
    "inference_model.build(tf.TensorShape([1, None]))\n",
    "inference_model.load_weights(f'{output_dir}/final_weights.h5')\n",
    "\n",
    "# Save Keras model\n",
    "keras_path = f'{output_dir}/text_generator.h5'\n",
    "inference_model.save(keras_path)\n",
    "print(f\"‚úÖ Saved Keras model\")\n",
    "\n",
    "# Convert to TFLite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(inference_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# iOS compatible\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS\n",
    "]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save TFLite\n",
    "tflite_path = f'{output_dir}/text_generator.tflite'\n",
    "with open(tflite_path, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(f\"‚úÖ Saved TFLite model\")\n",
    "print(f\"üìä Model size: {len(tflite_model) / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9Ô∏è‚É£ Test generation\n",
    "def generate_text(model, start_string, char2idx, idx2char, temperature=0.3):\n",
    "    num_generate = 200\n",
    "    input_eval = [char2idx.get(s, 0) for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    \n",
    "    text_generated = []\n",
    "    \n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        text_generated.append(idx2char.get(predicted_id, ''))\n",
    "    \n",
    "    return start_string + ''.join(text_generated)\n",
    "\n",
    "print(\"üß™ Testing generation...\\n\")\n",
    "test_prompts = [\"God \", \"I feel \", \"When I pray \"]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    generated = generate_text(inference_model, prompt, char2idx, idx2char)\n",
    "    print(f\"'{prompt}' -> {generated[:150]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéâ DOWNLOAD\n",
    "print(\"‚ú® COMPLETE!\\n\")\n",
    "print(\"üì¶ Models saved to Google Drive:\")\n",
    "print(f\"   {tflite_path}\")\n",
    "print(f\"   {output_dir}/char_vocab.txt\\n\")\n",
    "\n",
    "from google.colab import files\n",
    "print(\"üîΩ Auto-downloading to your computer...\")\n",
    "files.download(tflite_path)\n",
    "files.download(f'{output_dir}/char_vocab.txt')\n",
    "\n",
    "print(\"\\nüì± Copy to Flutter app:\")\n",
    "print(\"   assets/models/text_generator.tflite\")\n",
    "print(\"   assets/models/char_vocab.txt\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}