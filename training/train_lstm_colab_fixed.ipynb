{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ LSTM Christian Text Generator Training\n",
    "\n",
    "This notebook trains your LSTM model on Google Colab with FREE GPU acceleration.\n",
    "\n",
    "**Instructions:**\n",
    "1. Go to Runtime ‚Üí Change runtime type ‚Üí Select **T4 GPU**\n",
    "2. Run all cells in order\n",
    "3. Training will take ~2-3 hours\n",
    "4. Model will be saved to your Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£ Check GPU availability\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2Ô∏è‚É£ Mount Google Drive to save model\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create output directory\n",
    "import os\n",
    "output_dir = '/content/drive/MyDrive/everyday-christian-model'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Model will be saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3Ô∏è‚É£ Upload your training data file\n",
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "print(\"üì§ Please upload your training data file (lstm_training_data.txt)\")\n",
    "print(\"   File location on your Mac:\")\n",
    "print(\"   /Users/kcdacre8tor/everyday-christian/assets/training_data/lstm_training_data.txt\")\n",
    "print(\"\")\n",
    "print(\"Click the 'Choose Files' button that appears below:\")\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Move uploaded file to expected location\n",
    "for filename in uploaded.keys():\n",
    "    shutil.move(filename, '/content/training_data.txt')\n",
    "    file_size = len(uploaded[filename]) / (1024 * 1024)  # Convert to MB\n",
    "    print(f\"‚úÖ Uploaded {filename} ({file_size:.2f} MB)\")\n",
    "\n",
    "!ls -lh /content/training_data.txt\n",
    "print(\"‚úÖ Training data ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4Ô∏è‚É£ Training configuration and model code\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# PROPER Configuration for good results\n",
    "SEQ_LENGTH = 100\n",
    "BATCH_SIZE = 256  # GPU can handle larger batches\n",
    "BUFFER_SIZE = 10000\n",
    "EMBEDDING_DIM = 256\n",
    "RNN_UNITS = 1024  # Full size for GPU\n",
    "EPOCHS = 40  # Proper training (not 5!)\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "print(\"üìã Training Configuration:\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  RNN Units: {RNN_UNITS}\")\n",
    "print(f\"  Learning Rate: {LEARNING_RATE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5Ô∏è‚É£ Load and prepare data\n",
    "def prepare_data():\n",
    "    print(\"üìñ Loading training data...\")\n",
    "    with open('/content/training_data.txt', 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(text):,} characters\")\n",
    "    \n",
    "    # Create character mappings\n",
    "    vocab = sorted(set(text))\n",
    "    vocab = ['<PAD>', '<START>', '<END>'] + vocab\n",
    "    \n",
    "    char2idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "    idx2char = {idx: char for idx, char in enumerate(vocab)}\n",
    "    \n",
    "    print(f\"üìù Vocabulary size: {len(vocab)}\")\n",
    "    \n",
    "    # Save vocabulary\n",
    "    vocab_path = f'{output_dir}/char_vocab.txt'\n",
    "    with open(vocab_path, 'w', encoding='utf-8') as f:\n",
    "        for char in vocab:\n",
    "            f.write(f\"{char}\\n\")\n",
    "    \n",
    "    # Create sequences\n",
    "    text_as_int = np.array([char2idx.get(c, char2idx[' ']) for c in text])\n",
    "    char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "    sequences = char_dataset.batch(SEQ_LENGTH + 1, drop_remainder=True)\n",
    "    \n",
    "    def split_input_target(chunk):\n",
    "        input_text = chunk[:-1]\n",
    "        target_text = chunk[1:]\n",
    "        return input_text, target_text\n",
    "    \n",
    "    dataset = sequences.map(split_input_target)\n",
    "    dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "    dataset = dataset.cache().prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset, len(vocab), char2idx, idx2char\n",
    "\n",
    "# Prepare the data\n",
    "dataset, vocab_size, char2idx, idx2char = prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6Ô∏è‚É£ Build the LSTM model\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(\n",
    "            vocab_size,\n",
    "            embedding_dim,\n",
    "            batch_input_shape=[batch_size, None]\n",
    "        ),\n",
    "        tf.keras.layers.LSTM(\n",
    "            rnn_units,\n",
    "            return_sequences=True,\n",
    "            stateful=False,\n",
    "            recurrent_initializer='glorot_uniform',\n",
    "            recurrent_dropout=0.1\n",
    "        ),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.LSTM(\n",
    "            rnn_units // 2,\n",
    "            return_sequences=True,\n",
    "            stateful=False,\n",
    "            recurrent_initializer='glorot_uniform',\n",
    "            recurrent_dropout=0.1\n",
    "        ),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(\n",
    "            vocab_size,\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(0.01)\n",
    "        )\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Build model\n",
    "print(\"üèóÔ∏è Building LSTM model...\")\n",
    "model = build_model(vocab_size, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
    "\n",
    "# Compile\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7Ô∏è‚É£ TRAIN THE MODEL - This will take 2-3 hours\n",
    "print(\"üéì Starting training...\")\n",
    "print(\"‚è±Ô∏è This will take approximately 2-3 hours with T4 GPU\")\n",
    "print(\"‚òï Go grab a coffee! Colab will keep running.\\n\")\n",
    "\n",
    "# Callbacks\n",
    "checkpoint_path = f\"{output_dir}/checkpoint-{{epoch:02d}}\"\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        save_weights_only=True,\n",
    "        save_freq='epoch',\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='loss',\n",
    "        patience=5,\n",
    "        min_delta=0.001,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-5,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train!\n",
    "start_time = time.time()\n",
    "history = model.fit(\n",
    "    dataset,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "training_time = (time.time() - start_time) / 3600\n",
    "print(f\"\\n‚úÖ Training complete in {training_time:.2f} hours!\")\n",
    "print(f\"Final Loss: {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"Final Accuracy: {history.history['accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8Ô∏è‚É£ Rebuild model for inference and convert to TFLite\n",
    "print(\"üîÑ Rebuilding model for inference...\")\n",
    "inference_model = build_model(vocab_size, EMBEDDING_DIM, RNN_UNITS, batch_size=1)\n",
    "inference_model.set_weights(model.get_weights())\n",
    "inference_model.build(tf.TensorShape([1, None]))\n",
    "\n",
    "# Save Keras model\n",
    "keras_path = f'{output_dir}/text_generator.h5'\n",
    "inference_model.save(keras_path)\n",
    "print(f\"‚úÖ Saved Keras model to Google Drive\")\n",
    "\n",
    "# Convert to TFLite (iOS compatible)\n",
    "print(\"üì¶ Converting to TFLite...\")\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(inference_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# iOS compatible - CPU only operations\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS\n",
    "]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save TFLite model\n",
    "tflite_path = f'{output_dir}/text_generator.tflite'\n",
    "with open(tflite_path, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(f\"‚úÖ Saved TFLite model to Google Drive\")\n",
    "print(f\"üìä Model size: {len(tflite_model) / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9Ô∏è‚É£ Test the model\n",
    "def generate_text(model, start_string, char2idx, idx2char, temperature=0.3):\n",
    "    num_generate = 200\n",
    "    input_eval = [char2idx.get(s, char2idx.get(' ', 0)) for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    \n",
    "    text_generated = []\n",
    "    model.reset_states()\n",
    "    \n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "    \n",
    "    return start_string + ''.join(text_generated)\n",
    "\n",
    "# Test generation\n",
    "print(\"üß™ Testing text generation...\\n\")\n",
    "test_prompts = [\"God \", \"I feel \", \"When I pray \"]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    generated = generate_text(inference_model, prompt, char2idx, idx2char, temperature=0.3)\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    print(f\"Generated: {generated[:150]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéâ DOWNLOAD YOUR MODEL\n",
    "print(\"‚ú® Training Complete!\\n\")\n",
    "print(\"üì¶ Your trained models are saved in Google Drive:\")\n",
    "print(f\"   TFLite: {tflite_path}\")\n",
    "print(f\"   Vocabulary: {output_dir}/char_vocab.txt\\n\")\n",
    "print(\"To download to your computer:\")\n",
    "print(\"1. Go to Google Drive ‚Üí everyday-christian-model folder\")\n",
    "print(\"2. Download text_generator.tflite and char_vocab.txt\")\n",
    "print(\"3. Copy them to your Flutter app's assets/models/ folder\\n\")\n",
    "print(\"Or download directly from Colab:\")\n",
    "\n",
    "from google.colab import files\n",
    "print(\"\\nüîΩ Downloading model files to your computer...\")\n",
    "files.download(tflite_path)\n",
    "files.download(f'{output_dir}/char_vocab.txt')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}